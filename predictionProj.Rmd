---
title: 'Practical Machine Learning Prediction Project'
output: html_document

---
*Synopsis:* This analysis seeks to build a prediction model that correctly classifies exercise execution (dumbell curls, specifically), based on sensor readings from a fitness tracking device.  Using a random forest classifier as the underlying prediction model, and validating results via a cross-validation test set, we expect to be able to correctly classify exercise performance (either as executed correctly, or as one of four common error modes) over 90% of the time.

## Data Analysis

First, we need to set up our computing environment and ingest the input data.  We'll set aside 20% of the set for testing later, and investigate only the remaining training set.

```{r}
library(ggplot2); library(reshape2); library(caret); library(doParallel)
registerDoParallel(cores=4)
set.seed(1234)

setwd("/Users/tom/Desktop/JHU Data Science/8-machine learning/project")
d<-read.csv("pml-training.csv")

## TRUE for model building and data exploration, FALSE for full-up training and eval
MODEL.BUILD <- FALSE 
miniSet <- createDataPartition(y=d$classe, p=0.10, list=FALSE) 
if (MODEL.BUILD) d<-d[miniSet, ]  ## smaller data set for quick runs

## set aside observations for testing
inTraining <- createDataPartition(y=d$classe, p=0.8, list=FALSE)
d.train<-d[ inTraining,]
d.test <-d[-inTraining,]

## investigate the names and types of columns in the data
my.colnames <- as.character(names(d))
my.classes <- sapply(d, class)
my.info<-as.data.frame(cbind(my.colnames, my.classes))
```

In our training set, there are `r nrow(d.train)` total rows of `r ncol(d.train)` variable fields, one of which is the classification label for each row: `classe` (column 160), a factor with 5 levels: A B C D E.   

After investigating the `d.train` data.frame, it's clear we can exclude some of the columns from consideration as predictors.  The columns beginning with `max_`,  `min_`,  `avg_`,  `var_`,  `stddev_`,  `amplitude_`,  `skewness_`,  and `kurtosis_` all appear as prefixes for summary row entries... NA values elsewhere.  Also, other factor columns such as `user_name` and `new_window` (and other such non-quantitative factors) aren't useful either and can be removed from the model training set.

```{r}
## Make a logical vector corresponding to just the numeric or integer columns (except for c160)...
factor.cols <- my.info[,2]=="factor"; factor.cols[160]=FALSE
## ...and a logical vector that corresponds to the character prefixes on the summary stat columns
summary.cols <- grepl("max_", my.info$my.colnames) | grepl("min_",my.info$my.colnames) |
                grepl("avg_", my.info$my.colnames) | grepl("stddev_",my.info$my.colnames) |
                grepl("var_", my.info$my.colnames) | grepl("amplitude_",my.info$my.colnames) |
                grepl("skewness_", my.info$my.colnames) | grepl("kurtosis_",my.info$my.colnames) 
## our downsized training set will be just the columns that aren't "summary" or "factor"
d.train<-d.train[,!factor.cols & !summary.cols]
print(dim(d.train))
```

That leaves us with `r dim(d.train)[1]` observations of `r dim(d.train)[2]-1` predictors to use in model building.

## Modeling

Even by reducing the orginal data set's 160 columns down to `r dim(d.train)[2]`, we might be able to benefit from some additional pre-processing to help expedite the modeling process.  We'll employ principal component analysis for that, relying on the `caret` package defaults.  A random forest algorithm will be the underlying prediction model, as it is known as a capable performer for complex classification/prediction problems.

```{r,cache=TRUE}
## this is a classification problem - use a method appropriate for that: randomForest
mfit <- train(d.train$classe ~ . , method = "rf", data=d.train[,-57], preProcess="pca")
```

## Model Error Estimation Results

With any prediction model we build, we should be keenly interested in how we think it will perform when challenged with data it hasn't seen before.  We've set aside some of the provided data as "test" to use to estimate this out-of-sample error.  However, because we've chosen a random forest classifier as our underlying prediction model, we can exploit the way the algorithm trains.  Specifically, according to the creators of this classification method (Breiman and Cutler): https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr

*In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run... *

So we can leverage the training info to get an estimate of the out-of-sample misclassification rate.  `randomForest` calls this "OOB-error" ("out of bag").  The R model object (very conveniently!) retains the training results of each of the 500 classification trees `randomForest` constructed, allowing us to inspect the classifcation error rates. 

```{r}
print(summary(mfit$finalModel$err.rate))
errPlot <- ggplot(melt(as.data.frame(mfit$finalModel$err.rate))) +
           geom_histogram(aes(x=value,y=..density..*0.01),binwidth=0.01,color="white") +
           facet_wrap(~ variable) + theme_bw() + 
           labs(x="randomForest training misclassification rate", y="frequency")  + 
           ggtitle("Distribution of Misclassification Error Rate Estimates")
print(errPlot)

```

As an estimate for the out-of-sample error rate we might encounter in the wild, we'll use the mean of the OOB column as an upper bound: **`r round(mean(mfit$finalModel$err.rate[,"OOB"])*100,2)`%** of new observations could be misclassified on average, if they vary in the same way that the training data do.

However, if we don't trust (or don't have access to) the internal training error rates that `randomForest` maintained, we can also perform cross-validation of this model using the portion of data we set aside as `d.test` earlier, to gauge its performance via a confusion matrix and various statistics...

```{r}
## run the model against our test set, and see how well it does:
modelPredictions <- predict(mfit, newdata=d.test[,-160])
print(confusionMatrix(modelPredictions, d.test$classe))
```

Using our local set-aside test set `d.test`, which the model hasn't been exposed to during training, we see that it is correct for `r round(confusionMatrix(modelPredictions, d.test$classe)$overall[1]*100,2)`% of the `r nrow(d.test)` test observations it evaluated... an equivalent out-of-sample misclassifation rate of **`r 100-round(confusionMatrix(modelPredictions, d.test$classe)$overall[1]*100,2)`%**.  


 
